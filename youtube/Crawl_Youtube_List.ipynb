from bs4 import BeautifulSoup
from selenium import webdriver
import time
from selenium.webdriver.common.keys import Keys
from tqdm import tqdm
import pandas as pd
import re
import numpy as np

df = pd.read_excel("./sample2.xlsx") 
data_list = []

#URL = 'https://www.youtube.com/results?search_query=%EB%8F%84%EC%84%9C'
options = webdriver.ChromeOptions()
options.add_argument("no-sandbox")
options.add_argument("no-sandbox")
options.add_argument('window-size=1920x1080')
options.add_argument("disable-gpu")   
options.add_argument("lang=ko_KR")    
driver = webdriver.Chrome('./chromedriver.exe', options=options)
#driver.get(URL)

df_concat = pd.DataFrame()

for idx,row in tqdm(df.iterrows(), desc="크롤링"):
    search_name = row['fashion'] #칼럼 이름 작성
    print(search_name) 
    try:
        youtubeUrl = (f"https://www.youtube.com/results?search_query={search_name}") 
        driver.get(youtubeUrl) 
        print(youtubeUrl)
        time.sleep(0.1)
        no_of_pagedowns = 30
        elem = driver.find_element_by_tag_name("body") 
        
        while no_of_pagedowns:
            elem.send_keys(Keys.PAGE_DOWN)
            time.sleep(0.5)
            no_of_pagedowns -= 1
            time.sleep(1) #sleep을 통해 Robot Error 해결. 
            num = 0 
            try:
                driver.find_element_by_xpath("""//*[@id"feed-main-what_to_watch"]/button""").click()
            except:
                None
                
        html = driver.page_source
        
        soup = BeautifulSoup(html, 'lxml')
        titles = soup.find_all('h3')
        
        for title in titles:
            data_list.append(title.get_text().strip()) 
            print(title.get_text().strip())
        df = pd.DataFrame(data_list,columns=['text']) #유사시 데이터프레임 저장
        df_concat = pd.concat([df_concat,df])
    except: 
        print(idx) 
        
result_df = pd.DataFrame(data_list,columns=['title_name']) 
result_df.to_excel(f'./result__data.xlsx', index=False)


def cleanText(readData):
    text = re.sub('[-=+,#/\?:^$.@*\"※~&%ㆍ!』\\‘|\(\)\[\]\<\>`\'\\n…》]', '', readData)
    return text
    
this_data = result_df.drop_duplicates()['title_name'].apply(lambda x : cleanText(x))
this_data = this_data.apply(lambda x : x if len(x)>10 else np.nan)
this_data = this_data.dropna()
this_data.to_csv('fashion.csv',encoding='utf-8-sig',index=False)

